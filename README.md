ì „ê¸°ì°¨ ê°€ê²© ì˜ˆì¸¡ í•´ì»¤í†¤ : ë°ì´í„°ë¡œ EVë¥¼ ì½ë‹¤!ğŸš™
===============================================================
---------------------------------------------------------------

### ê°œë°œí™˜ê²½
```
Jupyter Notebook
python
```

<hr/>

### ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
```
import numpy as np
import pandas as pd
```

### ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
- ë°ì´í„° ì „ì²˜ë¦¬
1. í•™ìŠµ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„° ë¶„ë¦¬ : train ë°ì´í„°ì— í¬í•¨ëœ 'ê°€ê²©(ë°±ë§Œì›)' ë°ì´í„° ë¶„ë¦¬
```
train = pd.read_csv('data/final_train.csv')
test = pd.read_csv('data/final_test.csv')

# í•™ìŠµ ë° ì˜ˆì¸¡ ë°ì´í„° ë¶„ë¦¬
x_train = train.drop('ê°€ê²©(ë°±ë§Œì›)', axis = 1).values
y_train = train['ê°€ê²©(ë°±ë§Œì›)'].values
y_train = y_train.round().astype(int) # ì—°ì†í˜• ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜

x_test = test
```

2. í•™ìŠµ ë°ì´í„° ë¶„í•  : train ë°ì´í„°ë¥¼ í›ˆë ¨ìš© ë°ì´í„°(0.7)ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°(0.3)ë¡œ ë¶„í• 
```
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, train_size = 0.7,
                                                   random_state = 1, stratify = None)
```

3. í‘œì¤€í™”
- í•™ìŠµ ë°ì´í„° ê¸°ì¤€ í‘œì¤€í™” ì§„í–‰
```
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
sc.fit(x_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
```

### 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ ì´ˆëª¨ìˆ˜ íŠœë‹
- LogisticRegression : ì¼ë°˜í™” ì„ í˜• ëª¨ë¸. ë…ë¦½ ë³€ìˆ˜ì˜ ì„ í˜• ì¡°í•©ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢…ì† ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥  ì ìˆ˜ë¡œ ë³€í™˜.
- GridSearchCV : í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì œì‹œí•˜ì—¬, ê·¸ì¤‘ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ ì„ íƒ.
```
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

C_list = [0.01, 0.1, 1.0, 10.0, 100.0]
grid = {'C' : C_list}
logistic_gs = GridSearchCV(estimator = LogisticRegression(random_state = 1),
                          param_grid = grid,
                          scoring = 'accuracy', cv = 10, n_jobs = -1)
logistic_gs.fit(X_train_std, Y_train)

print(logistic_gs.best_score_)
print(logistic_gs.best_params_) # í•˜ì´í¼íŒŒë¼ë¯¸í„° C(10) ê°’ì„ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì— ì ìš©
```
   
### ëª¨ë¸ í›ˆë ¨
```
logistic = LogisticRegression(C = 10, random_state = 1)
logistic.fit(X_train_std, Y_train)
```

### 2. SVR êµì°¨ê²€ì¦
- SVR : Support Vector Regression. nì°¨ì› ê³µê°„ì—ì„œ ê° í´ë˜ìŠ¤ ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ìµœì ì˜ ì„  ë˜ëŠ” ì´ˆí‰ë©´ì„ ì°¾ì•„ ë°ì´í„°ë¥¼ ë¶„ë¥˜(ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ë§ˆì§„ì´ ìµœëŒ€ì¸ ì´ˆí‰ë©´ ì„ íƒ).
- GridSearchCV : í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì œì‹œí•˜ì—¬, ê·¸ì¤‘ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ ì„ íƒ.
```
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

grid = {'C' : [0.1, 1, 10, 100], 'degree' : np.arange(1, 11)}
svr_gs = GridSearchCV(estimator = SVR(kernel = 'poly', epsilon = 0.1, gamma = 'scale'),
                     param_grid = grid, scoring = 'r2', cv = 5)
svr_gs.fit(X_train_std, Y_train)

print(svr_gs.best_score_)
print(svr_gs.best_params_) # í•˜ì´í¼íŒŒë¼ë¯¸í„° C(100), degree(10) ê°’ì„ SVR ëª¨ë¸ì— ì ìš©
```
### NuSVR êµì°¨ê²€ì¦
```
from sklearn.svm import NuSVR
from sklearn.model_selection import GridSearchCV

grid = {'C' : [0.1, 1, 10, 100], 'degree' : np.arange(1, 11), 'nu' : [0.1, 0.2, 0.3, 0.4, 0.5]}
nusvr_gs = GridSearchCV(estimator = NuSVR(kernel = 'poly', gamma = 'scale'),
                     param_grid = grid, scoring = 'r2', cv = 5)
nusvr_gs.fit(X_train_std, Y_train)

print(nusvr_gs.best_score_)
print(nusvr_gs.best_params_) # í•˜ì´í¼íŒŒë¼ë¯¸í„° C(100), degree(10), nu(0.5) ê°’ì„ NuSVR ëª¨ë¸ì— ì ìš©
```

### ëª¨ë¸ í›ˆë ¨(SVR)
```
svr = SVR(kernel = 'poly', degree = 10, C = 100, epsilon = 0.1, gamma = 'scale')
svr.fit(X_train_std, Y_train)
```

### ëª¨ë¸ í›ˆë ¨(NuSVR)
```
nusvr = NuSVR(nu = 0.5, kernel = 'poly', degree = 10, C = 100, gamma = 'scale')
nusvr.fit(X_train_std, Y_train)
```

### 3. RandomForest êµì°¨ê²€ì¦
- RandomForest : ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°í•©(ê³¼ì í•© ê°‘ì†Œ + ì˜ˆì¸¡ ì„±ëŠ¥ í–¥ìƒ).
- GridSearchCV : í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì œì‹œí•˜ì—¬, ê·¸ì¤‘ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ ì„ íƒ.
```
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

rfr = RandomForestRegressor(0, n_jobs = -1)
params = {'n_estimators' : [10, 50, 100],
          'max_depth' : [6, 8, 10, 12, 14, 16],
          'min_samples_leaf' : [8, 12, 16, 18],
          'min_samples_split' : [8, 16, 20]}

rfr_gs = GridSearchCV(rfr, param_grid = params, cv = 5, n_jobs = -1)
rfr_gs.fit(X_train, Y_train)

print(rfr_gs.best_score_)
print(rfr_gs.best_params_) # {'max_depth': 16, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 100}
```

### ëª¨ë¸ í›ˆë ¨
```
frf_model = RandomForestRegressor(random_state = 0,
                                  max_depth = 16,
                                  min_samples_leaf = 8,
                                  min_samples_split = 20,
                                  n_estimators = 50)
frf_model.fit(X_train, Y_train)
```

### 4. XGBoost êµì°¨ê²€ì¦
- XGBoost : Extreme Gradient Boosting. ëª¨í˜•ë“¤ì˜ í•™ìŠµ ì—ëŸ¬ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ê³  ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ì— ë°˜ì˜í•˜ì—¬ ê°•í•œ ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±(ë³‘ë ¬ í•™ìŠµ ê°€ëŠ¥, ë¶„ë¥˜ ë° íšŒê·€ ëª¨ë‘ ì ìš© ê°€ëŠ¥).
```
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

xgbr = xgb.XGBRegressor()
params = {'n_estimators' : [100, 300, 500, 1000],
          'max_depth' : [10, 12, 14, 16],
          'learning_rate' : [0.1, 0.3, 0.5],
          'colsample_bytree' : [0.1, 0.3, 0.5, 0.7],
          'subsample' : [0.3, 0.5, 0.7],
          'min_child_weight' : [1, 3, 5, 7],
          'random_state' : [42],
          'n_jobs' : [-1]}

xgbr_gs = GridSearchCV(xgbr, param_grid = params, cv = 5, n_jobs = -1)
xgbr_gs.fit(X_train, Y_train)

print(xgbr_gs.best_score_)
print(xgbr_gs.best_params_)
# {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 7, 'n_estimators': 100, 'n_jobs': -1, 'random_state': 42, 'subsample': 0.7}
```

### ëª¨ë¸ í›ˆë ¨
```
xgbr_model = xgb.XGBRegressor(n_estimators = 100,
                              max_depth = 10,
                              learning_rate = 0.1,
                              colsample_bytree = 0.7,
                              subsample = 0.7,
                              min_child_weight = 7,
                              random_state = 42,
                              n_jobs = -1)
xgbr_model.fit(X_train, Y_train)
```

### 5. LightGBM êµì°¨ê²€ì¦
- LightGBM : ë¦¬í”„ ê¸°ì¤€ ë¶„í•  ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€ ì†ì‹¤ ê°’ì„ ê°–ëŠ” ë¦¬í”„ ë…¸ë“œë¥¼ ë¶„í• (ê¹Šê³  ë¹„ëŒ€ì¹­ì ì¸ íŠ¸ë¦¬ ìƒì„±). íŠ¸ë¦¬ ê¸°ì¤€ ë¶„í•  ë°©ì‹ë³´ë‹¤ ì˜ˆì¸¡ ì˜¤ë¥˜ ì†ì‹¤ ìµœì†Œí™” ê°€ëŠ¥.
```
import lightgbm as lgb
from sklearn.model_selection import GridSearchCV

lgbr = lgb.LGBMRegressor(num_leaves = 31, objective = 'regression', boosting = 'gbdt')
params = {'num_iterations' : [10, 50, 100],
          'learning_rate' : [0.1, 0.5],
          'max_depth' : [6, 8, 10, 12, 14, 16],
          'min_data_in_leaf' : [12, 16, 18, 20]}

lgbr_gs = GridSearchCV(lgbr, param_grid = params, cv = 5, n_jobs = -1)
lgbr_gs.fit(X_train, Y_train)

print(lgbr_gs.best_score_)
print(lgbr_gs.best_params_)
# {'learning_rate': 0.1, 'max_depth': 14, 'min_data_in_leaf': 20, 'num_iterations': 100}
```
### ëª¨ë¸ í›ˆë ¨
```
lgbr_model = lgb.LGBMRegressor(num_leaves = 31,
                               objective = 'regression',
                               boosting = 'gbdt',
                               max_depth = 14)
lgbr_model.fit(X_train, Y_train)
```

### 6. ë”¥ëŸ¬ë‹ ëª¨ë¸
1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
```
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, AlphaDropout, Dropout # Dense : ì€ë‹‰ì¸µ, Dropout and AlphaDropout : ì€ë‹‰ì¸µ ì‚¬ì´ ì‹ ê²½ë§ ëœë¤ ì œê±°(íš¨ìœ¨ì„± ì¦ê°€)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint # ëª¨ë¸ ì„±ëŠ¥ ë³€í™”ê°€ ê±°ì˜ ì—†ì„ ë•Œ í›ˆë ¨ ì¡°ê¸° ì¢…ë£Œ
from sklearn.model_selection import train_test_split # ë°ì´í„° ë¶„í• 
from sklearn.preprocessing import StandardScaler # í‘œì¤€í™”
from keras.layers import BatchNormalization # ë°°ì¹˜ ì •ê·œí™”
from keras.initializers import lecun_normal # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
```
2. í‘œì¤€í™” & ì •ê·œí™”
```
# í‘œì¤€í™”
scaler = StandardScaler()
X = scaler.fit_transform(X)

# ì •ê·œí™”
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
```

3. í™œì„±í™” í•¨ìˆ˜
- ReLU : ì…ë ¥ ê°’ì´ ì–‘ìˆ˜ì¼ ë•Œ ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥, ìŒìˆ˜ì¼ ë•Œ 0ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥(ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°). Dying ReLU í˜„ìƒ ë°œìƒ.
- ELU : Scaled ELU. ì¡°ê±´ì— ë”°ë¥¸ ìê¸° ì •ê·œí™”. ì¡°ê±´(ì…ë ¥ íŠ¹ì„± í‘œì¤€í™”, ëª¨ë“  ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ì— ë¥´ì¿¤ ì •ê·œë¶„í¬ ì´ˆê¸°í™” ì ìš©, ìˆœì°¨ ëª¨ë¸)
- Swish : Google ì—°êµ¬ì›ë“¤ì´ ê°œë°œí•œ ìì²´ ê²Œì´íŠ¸ í™œì„±í™” í•¨ìˆ˜. ì…ë ¥ ê°’ì´ ìŒìˆ˜ì¼ ë•Œì—ë„ í•™ìŠµ ê°€ëŠ¥(í•›ë¸Œ ê°€ì¤‘ì¹˜ì™€ ì…ë ¥ ë°ì´í„° í‘œí˜„ í–¥ìƒ).
4. ìµœì í™” ì•Œê³ ë¦¬ì¦˜(optimizer)ë¥¼ ì ìš©í•œ model.compile
- Nadam : Adam + NAG. íŒŒë¼ë¯¸í„° ê°±ì‹  ê³¼ì •ì—ì„œ ì´ì „ ë‹¨ê³„ì˜ Momentum(ê¸°ìš¸ê¸° ê°’ì´ 0ì¸ ê³³ì—ì„œë„ ê´€ì„±ì— ì˜í•´ ì—…ë°ì´íŠ¸ ìˆ˜í–‰ ê°€ëŠ¥) ëŒ€ì‹ , í˜„ì¬ Momentum ì‚¬ìš©(ë¯¸ë˜ì˜ Momentum ì‚¬ìš© íš¨ê³¼)
- Adadelta : í•™ìŠµë¥ ì„ ìë™ìœ¼ë¡œ ì¡°ì •í•˜ì—¬, ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì  ê°’ì„ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ì°¾ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜(ê¸°ìš¸ê¸° ëˆ„ì  ì œí•œ, ë™ì  í•™ìŠµë¥  ì¡°ì •).
5. ëª¨ë¸
- MLP : Multi-Layer Perceptron. ì§€ë„í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ì •ë°©í–¥ ì¸ê³µì‹ ê²½ë§. ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ì€ë‹‰ì¸µ í¬í•¨
- ìˆœì°¨ ëª¨ë¸ : Kerasì—ì„œ ì œê³µí•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì„± ë°©ì‹. Sequential()ì„ ì‚¬ìš©í•´ ìˆœì°¨ì  êµ¬ì¡° ëª¨ë¸ì„ ì‰½ê²Œ êµ¬ì„± ê°€ëŠ¥.
```
# ëª¨ë¸ (activation = relu, MinMaxScaler ì ìš©)
model = Sequential()
model.add(Dense(256, activation = 'relu', input_shape = (X_train.shape[1],)))
# model.add(Dropout(0.2))
model.add(Dense(128, activation = 'relu'))
# model.add(Dropout(0.2))
model.add(Dense(128, activation = 'relu'))
# model.add(Dropout(0.2))
model.add(Dense(64, activation = 'relu'))
# odel.add(Dropout(0.2))
model.add(Dense(1, activation = 'linear'))
model.summary()

# í•™ìŠµë‹¨ê³„ì—ì„œ loss(ì†ì‹¤í•¨ìˆ˜) ê³„ì‚°, í‰ê°€ë‹¨ê³„ì—ì„œ metrics(í‰ê°€ì§€í‘œ) ê³„ì‚°
model.compile(loss = 'mse', optimizer = 'AdaDelta', metrics = ['mae'])

estopping = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True, mode = 'min')
mcheckpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/ê³µëª¨ì „/model_best.keras', monitor = 'val_loss', save_best_only = True, mode =  'min')
```

```
# ëª¨ë¸2 (activation = elu, StandardScaler ì ìš©)
model_2 = Sequential()
model_2.add(Dense(256, activation = 'elu', input_shape = (X_train.shape[1],)))
# model_2.add(Dropout(0.2))
model_2.add(Dense(128, activation = 'elu'))
# model_2.add(Dropout(0.2))
model_2.add(Dense(128, activation = 'elu'))
# model_2.add(Dropout(0.2))
model_2.add(Dense(64, activation = 'elu'))
# model_2.add(Dropout(0.2))
model_2.add(Dense(1, activation = 'linear'))
model_2.summary()

# í•™ìŠµë‹¨ê³„ì—ì„œ loss(ì†ì‹¤í•¨ìˆ˜) ê³„ì‚°, í‰ê°€ë‹¨ê³„ì—ì„œ metrics(í‰ê°€ì§€í‘œ) ê³„ì‚°
model_2.compile(loss = 'mse', optimizer = 'Nadam', metrics = ['mae'])
```

```
# ëª¨ë¸3 (activation = selu(kernel_initializer = 'lecun_normal', AlphaDropout), StandardScaler ì ìš©)
model_3 = Sequential()
model_3.add(Dense(256, activation = 'selu', kernel_initializer = 'lecun_normal', input_shape = (X_train.shape[1],)))
# model_3.add(AlphaDropout(0.2))
model_3.add(Dense(128, activation = 'selu', kernel_initializer = 'lecun_normal'))
# model_3.add(AlphaDropout(0.2))
model_3.add(Dense(128, activation = 'selu', kernel_initializer = 'lecun_normal'))
# model_3.add(AlphaDropout(0.2))
model_3.add(Dense(64, activation = 'selu', kernel_initializer = 'lecun_normal'))
# model_3.add(AlphaDropout(0.2))
model_3.add(Dense(1, activation = 'linear'))
model_3.summary()

# í•™ìŠµë‹¨ê³„ì—ì„œ loss(ì†ì‹¤í•¨ìˆ˜) ê³„ì‚°, í‰ê°€ë‹¨ê³„ì—ì„œ metrics(í‰ê°€ì§€í‘œ) ê³„ì‚°
model_3.compile(loss = 'mse', optimizer = 'Nadam', metrics = ['mae'])
```

```
# ëª¨ë¸4 (activation = swish, StandardScaler ì ìš©)
model_4 = Sequential()
model_4.add(Dense(256, activation = 'swish', input_shape = (X_train.shape[1],)))
# model_4.add(Dropout(0.2))
model_4.add(Dense(128, activation = 'swish'))
# model_4.add(Dropout(0.2))
model_4.add(Dense(128, activation = 'swish'))
# model_4.add(Dropout(0.2))
model_4.add(Dense(64, activation = 'swish'))
# model_4.add(Dropout(0.2))
model_4.add(Dense(1, activation = 'linear'))
model_4.summary()

# í•™ìŠµë‹¨ê³„ì—ì„œ loss(ì†ì‹¤í•¨ìˆ˜) ê³„ì‚°, í‰ê°€ë‹¨ê³„ì—ì„œ metrics(í‰ê°€ì§€í‘œ) ê³„ì‚°
model_4.compile(loss = 'mse', optimizer = 'Nadam', metrics = ['mae'])
```

### ëª¨ë¸ í›ˆë ¨
```
history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 500, batch_size = 32, callbacks = [estopping, mcheckpoint])
```

```
history = model_2.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 500, batch_size = 32, callbacks = [estopping, mcheckpoint])
```

```
history = model_3.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 500, batch_size = 32, callbacks = [estopping, mcheckpoint], verbose = 1)
```

```
history = model_4.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 500, batch_size = 32, callbacks = [estopping, mcheckpoint], verbose = 1)
```

### ëª¨ë¸ ì •ë°€ë„ ì‹œê°í™”
- ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
1. MAPE : Mean Absolute Percentage Error(í‰ê·  ì ˆëŒ€ ë¹„ìœ¨ ì˜¤ì°¨). MAEë¥¼ ë¹„ìœ¨ë¡œ í‘œí˜„í•˜ì—¬ ìŠ¤ì¼€ì¼ ì˜ì¡´ì  ì—ëŸ¬ ë¬¸ì œì  ê°œì„ .
2. RMSE : Root Mean Squared Error(í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨). MSEì— ë£¨íŠ¸ë¥¼ ì”Œì›Œ ì™œê³¡ì„ ì¤„ì„.


![image](https://github.com/user-attachments/assets/f243bbdb-404e-4ab7-b829-efa7c4d5c5f1)
